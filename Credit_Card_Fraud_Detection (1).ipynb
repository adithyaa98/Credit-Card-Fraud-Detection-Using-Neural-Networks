{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uh9LSpJFeeMi"
   },
   "source": [
    "# Credit Card Fraud Detection using Neural Networks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nEp_saRJhH7-"
   },
   "source": [
    "## 1. Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5mT6pwImQTB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tF_c0C3pUmZ"
   },
   "source": [
    "## 2. Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "p1mQoDoMtmaX",
    "outputId": "9fcdb0bd-8dca-4cef-dfd0-a22009d8b1cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mounting google drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "KjzDCNylpcRq",
    "outputId": "3e351f5a-51df-45d7-e2fa-d68d51146a79"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
       "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
       "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
       "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
       "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
       "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading csv\n",
    "\n",
    "data = pd.read_csv(\"/content/drive/My Drive/creditcard.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "txrLWQbPp6Hn",
    "outputId": "a5836432-e27d-415a-d04d-63d8826a3c3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>1.768627e-15</td>\n",
       "      <td>9.170318e-16</td>\n",
       "      <td>-1.810658e-15</td>\n",
       "      <td>1.693438e-15</td>\n",
       "      <td>1.479045e-15</td>\n",
       "      <td>3.482336e-15</td>\n",
       "      <td>1.392007e-15</td>\n",
       "      <td>-7.528491e-16</td>\n",
       "      <td>4.328772e-16</td>\n",
       "      <td>9.049732e-16</td>\n",
       "      <td>5.085503e-16</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>1.088850e+00</td>\n",
       "      <td>1.020713e+00</td>\n",
       "      <td>9.992014e-01</td>\n",
       "      <td>9.952742e-01</td>\n",
       "      <td>9.585956e-01</td>\n",
       "      <td>9.153160e-01</td>\n",
       "      <td>8.762529e-01</td>\n",
       "      <td>8.493371e-01</td>\n",
       "      <td>8.381762e-01</td>\n",
       "      <td>8.140405e-01</td>\n",
       "      <td>7.709250e-01</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>-2.458826e+01</td>\n",
       "      <td>-4.797473e+00</td>\n",
       "      <td>-1.868371e+01</td>\n",
       "      <td>-5.791881e+00</td>\n",
       "      <td>-1.921433e+01</td>\n",
       "      <td>-4.498945e+00</td>\n",
       "      <td>-1.412985e+01</td>\n",
       "      <td>-2.516280e+01</td>\n",
       "      <td>-9.498746e+00</td>\n",
       "      <td>-7.213527e+00</td>\n",
       "      <td>-5.449772e+01</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>-5.354257e-01</td>\n",
       "      <td>-7.624942e-01</td>\n",
       "      <td>-4.055715e-01</td>\n",
       "      <td>-6.485393e-01</td>\n",
       "      <td>-4.255740e-01</td>\n",
       "      <td>-5.828843e-01</td>\n",
       "      <td>-4.680368e-01</td>\n",
       "      <td>-4.837483e-01</td>\n",
       "      <td>-4.988498e-01</td>\n",
       "      <td>-4.562989e-01</td>\n",
       "      <td>-2.117214e-01</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>-9.291738e-02</td>\n",
       "      <td>-3.275735e-02</td>\n",
       "      <td>1.400326e-01</td>\n",
       "      <td>-1.356806e-02</td>\n",
       "      <td>5.060132e-02</td>\n",
       "      <td>4.807155e-02</td>\n",
       "      <td>6.641332e-02</td>\n",
       "      <td>-6.567575e-02</td>\n",
       "      <td>-3.636312e-03</td>\n",
       "      <td>3.734823e-03</td>\n",
       "      <td>-6.248109e-02</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>4.539234e-01</td>\n",
       "      <td>7.395934e-01</td>\n",
       "      <td>6.182380e-01</td>\n",
       "      <td>6.625050e-01</td>\n",
       "      <td>4.931498e-01</td>\n",
       "      <td>6.488208e-01</td>\n",
       "      <td>5.232963e-01</td>\n",
       "      <td>3.996750e-01</td>\n",
       "      <td>5.008067e-01</td>\n",
       "      <td>4.589494e-01</td>\n",
       "      <td>1.330408e-01</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>2.374514e+01</td>\n",
       "      <td>1.201891e+01</td>\n",
       "      <td>7.848392e+00</td>\n",
       "      <td>7.126883e+00</td>\n",
       "      <td>1.052677e+01</td>\n",
       "      <td>8.877742e+00</td>\n",
       "      <td>1.731511e+01</td>\n",
       "      <td>9.253526e+00</td>\n",
       "      <td>5.041069e+00</td>\n",
       "      <td>5.591971e+00</td>\n",
       "      <td>3.942090e+01</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1  ...         Amount          Class\n",
       "count  284807.000000  2.848070e+05  ...  284807.000000  284807.000000\n",
       "mean    94813.859575  3.919560e-15  ...      88.349619       0.001727\n",
       "std     47488.145955  1.958696e+00  ...     250.120109       0.041527\n",
       "min         0.000000 -5.640751e+01  ...       0.000000       0.000000\n",
       "25%     54201.500000 -9.203734e-01  ...       5.600000       0.000000\n",
       "50%     84692.000000  1.810880e-02  ...      22.000000       0.000000\n",
       "75%    139320.500000  1.315642e+00  ...      77.165000       0.000000\n",
       "max    172792.000000  2.454930e+00  ...   25691.160000       1.000000\n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describing the data\n",
    "\n",
    "data.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "sVJevocDqW3b",
    "outputId": "8c5476df-e56d-49c3-9ec6-4c5abfe003b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Checking for any missing values\n",
    "\n",
    "data.info(null_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RePwG7n5ACt"
   },
   "source": [
    "## 3. Exploring the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzCVd8uj5TDf"
   },
   "outputs": [],
   "source": [
    "# Converting pandas dataframe to numpy array\n",
    "\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sHnMpVoI0inw",
    "outputId": "304afc7b-e33b-4707-f102-6d170bb77450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 30)\n",
      "(284807,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into input and output\n",
    "\n",
    "X = data[:,0:30] #input\n",
    "Y = data[:, 30] #output\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfuV2QfI3xw8"
   },
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sVe9v1nT8QXs",
    "outputId": "50afb832-b7f1-4789-effb-ab5b5661b91c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   624,    887,   1300,   1384,   2872,   3631,   3665,   5340,\n",
      "         5451,   6042,   6440,   6820,   7792,   8092,   8781,   9816,\n",
      "        10780,  10799,  10926,  10988,  11200,  11812,  12446,  12624,\n",
      "        13042,  13248,  13600,  14359,  15267,  15438,  15859,  17674,\n",
      "        18029,  18401,  18991,  19905,  20498,  20506,  20548,  20590,\n",
      "        21233,  21561,  23416,  23713,  24218,  24749,  25252,  25334,\n",
      "        26177,  26313,  26472,  26740,  27585,  27598,  27724,  27914,\n",
      "        29938,  30927,  31099,  31408,  32157,  33243,  33536,  33769,\n",
      "        34404,  36297,  36851,  36896,  37567,  37720,  37842,  38084,\n",
      "        38911,  39584,  39615,  40690,  41273,  41636,  41910,  42308,\n",
      "        42799,  43673,  44109,  44230,  44735,  44901,  45162,  45483,\n",
      "        45755,  45802,  46136,  46931,  46951,  47222,  47263,  47466,\n",
      "        48401,  49113,  49906,  50127,  50198,  50596,  50888,  52128,\n",
      "        53765,  54102,  54957,  55253,  55340,  55464,  56320,  56397,\n",
      "        56647,  58390,  58531,  59265,  59378,  59748,  60524,  60824,\n",
      "        61345,  61737,  62254,  64446,  65530,  66114,  66321,  67076,\n",
      "        67143,  67335,  68693,  69783,  70535,  70714,  70956,  71726,\n",
      "        71976,  72123,  72248,  72811,  73995,  74880,  75174,  75702,\n",
      "        75809,  75965,  76758,  77281,  77691,  78042,  79294,  81831,\n",
      "        81903,  82164,  82272,  82455,  82510,  83781,  84405,  84666,\n",
      "        85589,  86668,  86679,  87005,  87192,  87288,  87557,  87761,\n",
      "        87919,  88358,  89643,  90084,  90841,  91433,  92485,  92859,\n",
      "        93602,  94470,  94769,  96036,  97052,  98109,  98805,  98871,\n",
      "        98942,  99064,  99223, 101725, 102390, 102929, 102994, 103776,\n",
      "       104671, 104771, 108582, 108653, 109901, 109928, 111143, 111203,\n",
      "       111345, 113746, 113951, 115143, 115624, 116489, 116623, 117023,\n",
      "       117893, 119384, 119894, 120059, 120166, 120987, 122425, 122869,\n",
      "       122948, 122951, 123653, 123799, 123806, 123900, 124907, 125115,\n",
      "       125301, 125719, 127137, 127657, 128262, 128640, 129006, 129875,\n",
      "       131851, 132186, 132190, 133571, 133986, 134755, 135177, 135838,\n",
      "       137718, 138069, 138422, 139062, 139773, 140175, 140844, 141806,\n",
      "       142495, 142742, 142751, 143877, 144219, 144351, 145017, 145206,\n",
      "       146236, 147462, 147554, 147748, 147749, 149420, 150358, 150626,\n",
      "       150877, 151255, 151283, 151631, 152867, 153596, 157367, 157688,\n",
      "       157993, 158070, 158643, 160027, 160599, 160604, 160821, 161342,\n",
      "       161346, 164874, 165746, 166574, 167132, 167198, 167260, 167761,\n",
      "       169055, 169756, 173796, 174013, 175975, 176587, 176603, 176651,\n",
      "       176994, 180469, 180507, 180655, 181138, 181432, 183671, 183731,\n",
      "       183770, 183817, 184361, 185235, 186379, 186927, 186977, 187565,\n",
      "       188145, 188184, 189041, 189821, 190062, 192969, 193267, 193522,\n",
      "       193623, 193725, 194446, 195191, 195609, 195769, 195925, 196122,\n",
      "       196497, 196575, 197839, 199470, 199920, 200245, 200550, 201047,\n",
      "       202621, 202858, 203808, 204213, 204416, 204548, 205002, 205042,\n",
      "       205105, 205262, 205986, 206257, 206724, 206846, 207147, 207210,\n",
      "       207967, 208164, 208716, 209411, 209510, 211505, 212186, 212282,\n",
      "       212476, 213180, 213234, 213494, 213567, 213934, 215077, 215159,\n",
      "       215274, 215874, 216021, 216157, 216658, 216976, 217099, 218764,\n",
      "       219836, 221311, 221479, 221668, 222109, 222170, 222405, 223092,\n",
      "       223477, 223602, 223643, 223657, 224113, 224679, 225683, 225774,\n",
      "       226413, 226609, 227266, 227414, 227486]),)\n",
      "(array([  534,  1095,  2748,  3777,  4985,  5625,  6886,  8478,  8877,\n",
      "        9288,  9688,  9699,  9877, 10576, 10724, 11418, 11769, 11869,\n",
      "       12211, 12783, 13417, 13688, 14367, 14623, 14641, 14756, 15005,\n",
      "       15501, 15738, 16108, 17276, 17431, 18738, 19245, 19420, 19758,\n",
      "       20483, 20681, 21109, 21941, 22468, 22821, 23058, 23847, 24126,\n",
      "       25066, 25984, 27124, 27522, 29263, 29766, 31325, 31646, 31995,\n",
      "       33495, 34604, 34676, 34929, 35679, 35772, 36005, 36010, 37521,\n",
      "       39435, 40149, 42368, 42466, 42566, 43003, 43375, 44655, 45207,\n",
      "       46044, 46565, 46987, 47246, 47338, 47613, 47879, 48869, 48910,\n",
      "       49608, 49618, 50736, 50886, 50914, 51111, 52556, 53386, 53814,\n",
      "       54275, 54751, 55581, 55776, 56446]),)\n"
     ]
    }
   ],
   "source": [
    "# Checking to see if there are fraud values in both sets\n",
    "\n",
    "print(np.where(Y_train == 1))\n",
    "print(np.where(Y_test == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "CTYa0qJ888oq",
    "outputId": "fe78557c-d5cf-40cb-9396-0d97706b813a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training Examples : 227845\n",
      "No of test Examples : 56962\n",
      "Shape of training data : (227845, 30)\n",
      "Shape of test data : (56962, 30)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of the newly formed arrays\n",
    "\n",
    "m_train = X_train.shape[0]\n",
    "m_test = X_test.shape[0]\n",
    "\n",
    "print(\"No of training Examples : \"+str(m_train))   \n",
    "print(\"No of test Examples : \"+str(m_test))       \n",
    "print(\"Shape of training data : \"+str(X_train.shape))\n",
    "print(\"Shape of test data : \"+str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "4gmh-t30Arf-",
    "outputId": "9ce24734-0788-4491-ea1d-ea5d3a42222c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training Examples : (30, 227845)\n",
      "No of test Examples : (1, 227845)\n",
      "No of X_test Examples : (30, 56962)\n",
      "No of Y_test Examples : (1, 56962)\n"
     ]
    }
   ],
   "source": [
    "# Flattening the data\n",
    "\n",
    "X_train_flatten = X_train.reshape(X_train.shape[0],-1).T\n",
    "Y_train_flatten = Y_train.reshape(Y_train.shape[0],-1).T\n",
    "X_test_flatten = X_test.reshape(X_test.shape[0],-1).T\n",
    "Y_test_flatten = Y_test.reshape(Y_test.shape[0],-1).T\n",
    "\n",
    "print(\"No of training Examples : \"+str(X_train_flatten.shape))  \n",
    "print(\"No of test Examples : \"+str(Y_train_flatten.shape)) \n",
    "print(\"No of X_test Examples : \"+str(X_test_flatten.shape))  \n",
    "print(\"No of Y_test Examples : \"+str(Y_test_flatten.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "n3_zLBqB3ZX_",
    "outputId": "d12b9da7-a96b-41b8-b43e-e3dd9a694db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of X_train_set shape : (30, 227845)\n",
      "No of Y_train_set shape : (1, 227845)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the data\n",
    "\n",
    "X_train_set = normalize(X_train_flatten)\n",
    "Y_train_set = Y_train_flatten\n",
    "\n",
    "print(\"No of X_train_set shape : \"+str(X_train_set.shape))  \n",
    "print(\"No of Y_train_set shape : \"+str(Y_train_set.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7R8rDxiLGMb"
   },
   "source": [
    "## 4. Initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ukISXQ83LKt2"
   },
   "outputs": [],
   "source": [
    "# We use random initialization for weights\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1,L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "Vup0EUIDNZW_",
    "outputId": "4f884592-abba-4d62-c637-7dc83d8b6d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 9.52138462e-04 -2.93140734e-03  2.43061642e-03  6.34565102e-03\n",
      "   8.65531932e-03]\n",
      " [-1.96607990e-03  6.08357421e-04 -2.11062301e-02 -2.44369533e-02\n",
      "   5.51137518e-03]\n",
      " [-1.85441045e-02 -6.41637874e-03 -6.18441411e-03  3.80980312e-03\n",
      "  -9.00057833e-05]\n",
      " [ 2.84785895e-03 -4.20824564e-03 -1.85349411e-03  9.47290877e-03\n",
      "  -1.23128916e-03]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.02063284  0.0018386  -0.00089302  0.00849966]\n",
      " [-0.00021157  0.00882069  0.01084461 -0.00754515]\n",
      " [-0.00102871 -0.00568433  0.00143861  0.00800822]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Testing if the function works\n",
    "\n",
    "parameters = initialize_parameters([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZEHkuzaOPm_"
   },
   "source": [
    "## 5. Creating Activation Functions and their Derivation Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPxxL2BhOURP"
   },
   "outputs": [],
   "source": [
    "# Sigmoid Function\n",
    "\n",
    "def sigmoid(z):\n",
    "  \n",
    "    s = 1/(1+np.exp(-z))\n",
    "    cache = z\n",
    "    return s,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jIHuX9pkOohW",
    "outputId": "562179c9-ef42-4233-83e8-5bf10ac83cfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88079708, 0.95257413, 0.99330715, 0.99908895]), array([2, 3, 5, 7]))"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing that it works\n",
    "\n",
    "sigmoid(np.array(([2,3,5,7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "is3ELqmmO57e"
   },
   "outputs": [],
   "source": [
    "# ReLu Function\n",
    "\n",
    "def relu(z):\n",
    "    \n",
    "    r = np.maximum(0,z)\n",
    "    cache = z\n",
    "    return r,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pxVuUZy7PIYR",
    "outputId": "997aca24-8775-495a-c09f-8ad691c53b4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 0, 5, 0]), array([ 2, -3,  5, -7]))"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing that it works\n",
    "\n",
    "relu(np.array([2,-3,5,-7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-LwE3LGlldP"
   },
   "outputs": [],
   "source": [
    "# Relu Backward Function\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8T5yvIgTtWbm"
   },
   "outputs": [],
   "source": [
    "# Sigmoid Backward Function\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "           \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhSVAKYffOyz"
   },
   "source": [
    "## 6. Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cr4_lVnbfTOe"
   },
   "outputs": [],
   "source": [
    "# Linear Forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHfCPdXcf1PB"
   },
   "outputs": [],
   "source": [
    "# Linear Activation Forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I9ZOIwavgAzq"
   },
   "outputs": [],
   "source": [
    "# L Layer Forward Propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A, cache = linear_activation_forward(A,parameters[\"W\" + str(l)],parameters[\"b\" + str(l)],activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A,parameters[\"W\" + str(L)],parameters[\"b\" + str(L)],activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KNyL1Go3gU0w"
   },
   "source": [
    "## 7. Cost Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxLdgd9XgXLV"
   },
   "outputs": [],
   "source": [
    "def cost_function(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = (-1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NN63EVEvjW8W"
   },
   "source": [
    "## 8. Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJSzkMVmjbOm"
   },
   "outputs": [],
   "source": [
    "# Linear Backward \n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6rdbKcujqGi"
   },
   "outputs": [],
   "source": [
    "# Linear Activation Backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_2EZEGhwXKS"
   },
   "outputs": [],
   "source": [
    "# Backward Propagation\n",
    "\n",
    "def backward_propagation(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache,activation=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+2)],current_cache,activation=\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "md-6emEbyI9q"
   },
   "source": [
    "## 9. Updating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9Nlz70RyNJS"
   },
   "outputs": [],
   "source": [
    "# Update Parameters \n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    for l in range(1,L+1):\n",
    "        parameters[\"W\"+str(l)]=parameters[\"W\" + str(l)]-learning_rate*grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\"+str(l)]=parameters[\"b\" + str(l)]-learning_rate*grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvBPe-gryjw2"
   },
   "source": [
    "## 10. Creating the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GV-nut1Xyn5c"
   },
   "outputs": [],
   "source": [
    "def nn_model(X,Y,layer_dims,learning_rate=0.01, num_iterations=5000,print_cost=False):\n",
    "    costs = []\n",
    "    \n",
    "    #initialize parameters \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    # for loop for iterations/epoch \n",
    "    for i in range(0,num_iterations):\n",
    "        #forward_propagation\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "        #compute cost\n",
    "        cost = cost_function(AL, Y)\n",
    "        \n",
    "        #backward_propagation \n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    "        \n",
    "        #update parameters\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UCZI0d12FQjR"
   },
   "source": [
    "## 11. Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eNDE4sKG0qga",
    "outputId": "7d280597-2abf-4adc-a60a-dd08f9d11707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.497206\n",
      "Cost after iteration 200: 0.375556\n",
      "Cost after iteration 300: 0.296549\n",
      "Cost after iteration 400: 0.242656\n",
      "Cost after iteration 500: 0.204224\n",
      "Cost after iteration 600: 0.175755\n",
      "Cost after iteration 700: 0.153986\n",
      "Cost after iteration 800: 0.136893\n",
      "Cost after iteration 900: 0.123168\n",
      "Cost after iteration 1000: 0.111937\n",
      "Cost after iteration 1100: 0.102599\n",
      "Cost after iteration 1200: 0.094725\n",
      "Cost after iteration 1300: 0.088005\n",
      "Cost after iteration 1400: 0.082210\n",
      "Cost after iteration 1500: 0.077166\n",
      "Cost after iteration 1600: 0.072739\n",
      "Cost after iteration 1700: 0.068825\n",
      "Cost after iteration 1800: 0.065342\n",
      "Cost after iteration 1900: 0.062224\n",
      "Cost after iteration 2000: 0.059417\n",
      "Cost after iteration 2100: 0.056878\n",
      "Cost after iteration 2200: 0.054572\n",
      "Cost after iteration 2300: 0.052467\n",
      "Cost after iteration 2400: 0.050541\n",
      "Cost after iteration 2500: 0.048770\n",
      "Cost after iteration 2600: 0.047138\n",
      "Cost after iteration 2700: 0.045628\n",
      "Cost after iteration 2800: 0.044229\n",
      "Cost after iteration 2900: 0.042928\n",
      "Cost after iteration 3000: 0.041716\n",
      "Cost after iteration 3100: 0.040584\n",
      "Cost after iteration 3200: 0.039524\n",
      "Cost after iteration 3300: 0.038530\n",
      "Cost after iteration 3400: 0.037596\n",
      "Cost after iteration 3500: 0.036718\n",
      "Cost after iteration 3600: 0.035889\n",
      "Cost after iteration 3700: 0.035106\n",
      "Cost after iteration 3800: 0.034366\n",
      "Cost after iteration 3900: 0.033665\n",
      "Cost after iteration 4000: 0.033000\n",
      "Cost after iteration 4100: 0.032369\n",
      "Cost after iteration 4200: 0.031769\n",
      "Cost after iteration 4300: 0.031198\n",
      "Cost after iteration 4400: 0.030653\n",
      "Cost after iteration 4500: 0.030134\n",
      "Cost after iteration 4600: 0.029638\n",
      "Cost after iteration 4700: 0.029164\n",
      "Cost after iteration 4800: 0.028710\n",
      "Cost after iteration 4900: 0.028276\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_dims = [30,25,20,15,10,5,1]\n",
    "\n",
    "parameters = nn_model(X_train_set,Y_train_set,layer_dims,learning_rate=0.01,num_iterations = 5000, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-aXI4_9FVp4"
   },
   "source": [
    "## 12. Predict Function and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WK7f1_TD1MC5"
   },
   "outputs": [],
   "source": [
    "# Predict function\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probablities, caches = forward_propagation(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probablities to 0/1 predictions\n",
    "    for i in range(0, probablities.shape[1]):\n",
    "        if probablities[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TWMAep8rFxR9",
    "outputId": "e60b382c-4716-479a-fc8b-131398c922a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9982575873949391\n"
     ]
    }
   ],
   "source": [
    "# Training Set Accuracy\n",
    "\n",
    "pred_train = predict(X_train_set, Y_train_set, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uEloGTLBF2Ik",
    "outputId": "647bd981-cee5-4958-f362-2162265c9e98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9983322214809873\n"
     ]
    }
   ],
   "source": [
    "# Test Set Accuracy\n",
    "\n",
    "pred_test = predict(X_test_flatten, Y_test_flatten, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWXFlchPF7dC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Credit-Card-Fraud-Detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
