{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uh9LSpJFeeMi"
   },
   "source": [
    "# Credit Card Fraud Detection using Neural Networks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nEp_saRJhH7-"
   },
   "source": [
    "## 1. Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5mT6pwImQTB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tF_c0C3pUmZ"
   },
   "source": [
    "## 2. Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "KjzDCNylpcRq",
    "outputId": "3e351f5a-51df-45d7-e2fa-d68d51146a79"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading csv\n",
    "\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "txrLWQbPp6Hn",
    "outputId": "a5836432-e27d-415a-d04d-63d8826a3c3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describing the data\n",
    "\n",
    "data.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "sVJevocDqW3b",
    "outputId": "8c5476df-e56d-49c3-9ec6-4c5abfe003b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Checking for any missing values\n",
    "\n",
    "data.info(null_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RePwG7n5ACt"
   },
   "source": [
    "## 3. Exploring the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzCVd8uj5TDf"
   },
   "outputs": [],
   "source": [
    "# Converting pandas dataframe to numpy array\n",
    "\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sHnMpVoI0inw",
    "outputId": "304afc7b-e33b-4707-f102-6d170bb77450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 30)\n",
      "(284807, 1)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into input and output\n",
    "\n",
    "X = data[:,0:30] #input\n",
    "Y = data[:, 30] #output\n",
    "Y = Y.reshape(-1,1)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfuV2QfI3xw8"
   },
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sVe9v1nT8QXs",
    "outputId": "50afb832-b7f1-4789-effb-ab5b5661b91c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   417,    669,   1305,   1319,   1689,   2476,   3799,   4074,\n",
      "         4526,   5567,   7007,   7081,   8301,  12345,  12976,  13073,\n",
      "        13658,  14035,  14120,  14305,  14510,  15506,  15886,  16108,\n",
      "        17974,  18214,  19848,  20274,  20392,  20665,  21946,  22800,\n",
      "        23695,  23967,  24927,  25293,  25470,  25942,  26503,  26620,\n",
      "        28016,  28317,  28506,  28778,  28823,  28982,  29145,  30216,\n",
      "        30298,  31644,  31851,  32175,  32506,  32711,  32737,  33000,\n",
      "        33026,  33338,  33396,  35225,  35455,  35475,  36674,  37950,\n",
      "        38948,  38992,  39838,  39884,  40251,  42455,  42460,  43130,\n",
      "        44239,  45511,  45797,  48267,  49613,  50823,  51077,  51811,\n",
      "        52373,  53331,  53904,  54767,  54794,  55382,  56040,  56745,\n",
      "        56794,  57062,  57998,  58193,  58698,  59833,  60121,  60711,\n",
      "        61548,  62689,  62963,  64065,  64399,  66048,  67041,  67655,\n",
      "        67951,  68178,  68692,  69520,  70153,  70396,  70557,  72259,\n",
      "        72528,  72673,  73048,  73065,  73403,  73641,  73661,  73847,\n",
      "        74259,  75212,  75353,  75358,  76127,  77207,  77644,  77693,\n",
      "        78124,  78447,  78602,  78647,  78648,  80005,  80366,  80658,\n",
      "        82244,  82274,  82417,  82892,  82965,  83425,  84566,  84651,\n",
      "        84975,  86319,  86679,  87415,  87745,  88712,  89620,  90072,\n",
      "        90145,  90593,  90784,  91907,  92705,  93235,  93994,  94390,\n",
      "        96193,  98169,  98366, 100731, 101159, 101783, 102780, 103194,\n",
      "       103512, 103631, 104272, 105348, 105988, 105992, 106125, 106128,\n",
      "       106405, 107366, 107615, 107914, 108339, 108740, 109417, 111626,\n",
      "       112346, 112874, 113059, 113076, 113688, 113718, 114791, 114889,\n",
      "       115440, 116156, 117128, 117581, 118392, 118901, 119145, 119260,\n",
      "       119793, 119862, 121241, 121307, 121998, 122709, 123227, 123468,\n",
      "       124180, 124482, 125153, 125677, 126961, 127333, 128027, 128795,\n",
      "       128910, 129517, 131133, 131349, 131826, 133110, 133909, 134715,\n",
      "       134737, 136135, 136900, 137474, 137927, 138226, 138252, 138694,\n",
      "       140054, 140875, 141393, 141600, 142453, 143401, 143949, 144194,\n",
      "       144482, 144603, 145300, 145411, 145610, 146014, 146206, 148514,\n",
      "       148624, 149243, 150321, 150686, 152764, 153117, 153224, 153422,\n",
      "       154612, 154802, 155333, 156230, 158359, 159284, 159847, 160150,\n",
      "       160162, 160791, 160962, 161068, 162081, 162625, 163290, 163311,\n",
      "       164373, 165632, 165659, 166424, 166501, 166701, 168207, 169095,\n",
      "       169825, 169950, 170078, 170166, 170895, 171103, 171711, 171817,\n",
      "       172909, 173558, 173892, 174170, 175381, 176772, 176917, 177298,\n",
      "       178518, 179137, 179731, 180046, 181799, 181904, 181960, 182791,\n",
      "       182859, 182955, 183025, 183864, 183969, 184140, 184765, 185236,\n",
      "       186817, 188913, 189226, 189232, 189552, 189849, 190858, 190874,\n",
      "       190967, 191208, 192570, 192877, 193662, 193826, 194097, 194375,\n",
      "       194962, 194975, 197035, 197696, 198361, 199119, 200025, 200926,\n",
      "       200954, 202039, 202056, 202181, 203219, 204364, 204448, 204710,\n",
      "       205536, 205891, 207354, 208894, 209612, 210960, 211188, 211264,\n",
      "       212552, 212691, 212817, 214229, 214781, 214836, 215442, 215800,\n",
      "       216278, 217292, 218053, 219683, 220038, 221084, 221498, 222844,\n",
      "       223105, 223123, 224193, 224719, 224754, 225098, 225217, 225558,\n",
      "       225794, 226147, 227213, 227269, 227584], dtype=int64), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int64))\n",
      "(array([   49,   146,   448,  1023,  1378,  1669,  1850,  3051,  3966,\n",
      "        4692,  5058,  5147,  5577,  5824,  5947,  5978,  6671,  7181,\n",
      "        7288,  7644,  7952,  8795, 10968, 11299, 11990, 12349, 12546,\n",
      "       12610, 14096, 14490, 14673, 14852, 14876, 15547, 15852, 16294,\n",
      "       16480, 16649, 17828, 17966, 18829, 19217, 19362, 19515, 20086,\n",
      "       20115, 20170, 20375, 21033, 21470, 21964, 22893, 23594, 24267,\n",
      "       24360, 24965, 25167, 26389, 26847, 28364, 28931, 29283, 30037,\n",
      "       30125, 30798, 31470, 31581, 32489, 33717, 33740, 34034, 34116,\n",
      "       34538, 35103, 35161, 35303, 36699, 37068, 38747, 41493, 41520,\n",
      "       41575, 42089, 42174, 42944, 44083, 44347, 44869, 45661, 45676,\n",
      "       45956, 46463, 48361, 48990, 49130, 49528, 50131, 50573, 50626,\n",
      "       50760, 52123, 52229, 52494, 52592, 52791, 52808, 53145, 54472,\n",
      "       54539, 55240, 56421], dtype=int64), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Checking to see if there are fraud values in both sets\n",
    "\n",
    "print(np.where(Y_train == 1))\n",
    "print(np.where(Y_test == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "CTYa0qJ888oq",
    "outputId": "fe78557c-d5cf-40cb-9396-0d97706b813a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training Examples : 227845\n",
      "No of test Examples : 56962\n",
      "Shape of training data : (227845, 30)\n",
      "Shape of test data : (56962, 30)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of the newly formed arrays\n",
    "\n",
    "m_train = X_train.shape[0]\n",
    "m_test = X_test.shape[0]\n",
    "\n",
    "print(\"No of training Examples : \"+str(m_train))   \n",
    "print(\"No of test Examples : \"+str(m_test))       \n",
    "print(\"Shape of training data : \"+str(X_train.shape))\n",
    "print(\"Shape of test data : \"+str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "n3_zLBqB3ZX_",
    "outputId": "d12b9da7-a96b-41b8-b43e-e3dd9a694db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of X_train_set shape : (30, 227845)\n",
      "No of Y_train_set shape : (1, 227845)\n",
      "No of X_test_set shape : (30, 56962)\n",
      "No of Y_test_set shape : (1, 56962)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the data\n",
    "\n",
    "X_train_set = normalize(X_train)\n",
    "X_test_set = normalize(X_test)\n",
    "Y_train_set = Y_train\n",
    "Y_test_set = Y_test\n",
    "X_train_set = X_train_set.T\n",
    "Y_train_set = Y_train_set.T\n",
    "X_test_set = X_test_set.T\n",
    "Y_test_set = Y_test_set.T\n",
    "\n",
    "print(\"No of X_train_set shape : \"+str(X_train_set.shape))  \n",
    "print(\"No of Y_train_set shape : \"+str(Y_train_set.shape)) \n",
    "print(\"No of X_test_set shape : \"+str(X_test_set.shape))  \n",
    "print(\"No of Y_test_set shape : \"+str(Y_test_set.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7R8rDxiLGMb"
   },
   "source": [
    "## 4. Initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ukISXQ83LKt2"
   },
   "outputs": [],
   "source": [
    "# We use random initialization for weights\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1,L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "Vup0EUIDNZW_",
    "outputId": "4f884592-abba-4d62-c637-7dc83d8b6d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.0155558  -0.01789823  0.00343435  0.0045021   0.00329367]\n",
      " [-0.0091871  -0.01030201 -0.00518279 -0.00676086 -0.00149245]\n",
      " [-0.00376665 -0.0148901  -0.00948874  0.00161453 -0.00345256]\n",
      " [-0.00523165  0.00954874 -0.00549534 -0.00324952  0.00482907]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01996759 -0.01005391  0.0070481   0.00222514]\n",
      " [ 0.00583346 -0.00365089  0.00925419 -0.00146596]\n",
      " [-0.00473493 -0.00683973 -0.01633994 -0.01631817]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Testing if the function works\n",
    "\n",
    "parameters = initialize_parameters([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZEHkuzaOPm_"
   },
   "source": [
    "## 5. Creating Activation Functions and their Derivation Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPxxL2BhOURP"
   },
   "outputs": [],
   "source": [
    "# Sigmoid Function\n",
    "\n",
    "def sigmoid(z):\n",
    "  \n",
    "    s = 1/(1+np.exp(-z))\n",
    "    cache = z\n",
    "    return s,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jIHuX9pkOohW",
    "outputId": "562179c9-ef42-4233-83e8-5bf10ac83cfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88079708, 0.95257413, 0.99330715, 0.99908895]), array([2, 3, 5, 7]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing that it works\n",
    "\n",
    "sigmoid(np.array(([2,3,5,7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "is3ELqmmO57e"
   },
   "outputs": [],
   "source": [
    "# ReLu Function\n",
    "\n",
    "def relu(z):\n",
    "    \n",
    "    r = np.maximum(0,z)\n",
    "    cache = z\n",
    "    return r,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pxVuUZy7PIYR",
    "outputId": "997aca24-8775-495a-c09f-8ad691c53b4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 0, 5, 0]), array([ 2, -3,  5, -7]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing that it works\n",
    "\n",
    "relu(np.array([2,-3,5,-7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-LwE3LGlldP"
   },
   "outputs": [],
   "source": [
    "# Relu Backward Function\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8T5yvIgTtWbm"
   },
   "outputs": [],
   "source": [
    "# Sigmoid Backward Function\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "           \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhSVAKYffOyz"
   },
   "source": [
    "## 6. Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cr4_lVnbfTOe"
   },
   "outputs": [],
   "source": [
    "# Linear Forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHfCPdXcf1PB"
   },
   "outputs": [],
   "source": [
    "# Linear Activation Forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I9ZOIwavgAzq"
   },
   "outputs": [],
   "source": [
    "# L Layer Forward Propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A, cache = linear_activation_forward(A,parameters[\"W\" + str(l)],parameters[\"b\" + str(l)],activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A,parameters[\"W\" + str(L)],parameters[\"b\" + str(L)],activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KNyL1Go3gU0w"
   },
   "source": [
    "## 7. Cost Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxLdgd9XgXLV"
   },
   "outputs": [],
   "source": [
    "def cost_function(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = (-1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NN63EVEvjW8W"
   },
   "source": [
    "## 8. Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJSzkMVmjbOm"
   },
   "outputs": [],
   "source": [
    "# Linear Backward \n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6rdbKcujqGi"
   },
   "outputs": [],
   "source": [
    "# Linear Activation Backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_2EZEGhwXKS"
   },
   "outputs": [],
   "source": [
    "# Backward Propagation\n",
    "\n",
    "def backward_propagation(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache,activation=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+2)],current_cache,activation=\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "md-6emEbyI9q"
   },
   "source": [
    "## 9. Updating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9Nlz70RyNJS"
   },
   "outputs": [],
   "source": [
    "# Update Parameters \n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    for l in range(1,L+1):\n",
    "        parameters[\"W\"+str(l)]=parameters[\"W\" + str(l)]-learning_rate*grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\"+str(l)]=parameters[\"b\" + str(l)]-learning_rate*grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvBPe-gryjw2"
   },
   "source": [
    "## 10. Creating the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GV-nut1Xyn5c"
   },
   "outputs": [],
   "source": [
    "def nn_model(X,Y,layer_dims,learning_rate=0.01, num_iterations=5000,print_cost=False):\n",
    "    costs = []\n",
    "    \n",
    "    #initialize parameters \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    # for loop for iterations/epoch \n",
    "    for i in range(0,num_iterations):\n",
    "        #forward_propagation\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "        #compute cost\n",
    "        cost = cost_function(AL, Y)\n",
    "        \n",
    "        #backward_propagation \n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    "        \n",
    "        #update parameters\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UCZI0d12FQjR"
   },
   "source": [
    "## 11. Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eNDE4sKG0qga",
    "outputId": "7d280597-2abf-4adc-a60a-dd08f9d11707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.111140\n",
      "Cost after iteration 200: 0.059169\n",
      "Cost after iteration 300: 0.041698\n",
      "Cost after iteration 400: 0.033131\n",
      "Cost after iteration 500: 0.028104\n",
      "Cost after iteration 600: 0.024823\n",
      "Cost after iteration 700: 0.022528\n",
      "Cost after iteration 800: 0.020841\n",
      "Cost after iteration 900: 0.019554\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_dims = [30,25,20,15,10,5,1]\n",
    "\n",
    "parameters = nn_model(X_train_set,Y_train_set,layer_dims,learning_rate=0.1,num_iterations = 1000, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-aXI4_9FVp4"
   },
   "source": [
    "## 12. Predict Function and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WK7f1_TD1MC5"
   },
   "outputs": [],
   "source": [
    "# Predict function\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probablities, caches = forward_propagation(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probablities to 0/1 predictions\n",
    "    for i in range(0, probablities.shape[1]):\n",
    "        if probablities[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TWMAep8rFxR9",
    "outputId": "e60b382c-4716-479a-fc8b-131398c922a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9983278105729768\n"
     ]
    }
   ],
   "source": [
    "# Training Set Accuracy\n",
    "\n",
    "pred_train = predict(X_train_set, Y_train_set, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uEloGTLBF2Ik",
    "outputId": "647bd981-cee5-4958-f362-2162265c9e98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9980513324672589\n"
     ]
    }
   ],
   "source": [
    "# Test Set Accuracy\n",
    "\n",
    "pred_test = predict(X_test_set, Y_test_set, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWXFlchPF7dC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Credit-Card-Fraud-Detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
